{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Compact architectures\n## Audio Classification with the NBAC dataset\n\nThis notebook performs audio classification on audio fragments of 5 seconds long","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# IMPORT ALL LIBRARIES\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport random\nimport seaborn as sns\n\nimport librosa\nimport librosa.display\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow.keras.layers import Layer, Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:44:41.697071Z","iopub.execute_input":"2023-07-20T10:44:41.697510Z","iopub.status.idle":"2023-07-20T10:44:49.599723Z","shell.execute_reply.started":"2023-07-20T10:44:41.697462Z","shell.execute_reply":"2023-07-20T10:44:49.598609Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"# Set sample rate to work with at 8000\n\nSR = 16000","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:44:49.607460Z","iopub.execute_input":"2023-07-20T10:44:49.608261Z","iopub.status.idle":"2023-07-20T10:44:49.613961Z","shell.execute_reply.started":"2023-07-20T10:44:49.608225Z","shell.execute_reply":"2023-07-20T10:44:49.612697Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/DavidCastello/NBAC.git","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:44:49.615566Z","iopub.execute_input":"2023-07-20T10:44:49.615955Z","iopub.status.idle":"2023-07-20T10:45:16.926511Z","shell.execute_reply.started":"2023-07-20T10:44:49.615920Z","shell.execute_reply":"2023-07-20T10:45:16.925183Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'NBAC'...\nremote: Enumerating objects: 798, done.\u001b[K\nremote: Counting objects: 100% (151/151), done.\u001b[K\nremote: Compressing objects: 100% (149/149), done.\u001b[K\nremote: Total 798 (delta 4), reused 148 (delta 2), pack-reused 647\u001b[K\nReceiving objects: 100% (798/798), 474.50 MiB | 25.74 MiB/s, done.\nResolving deltas: 100% (42/42), done.\nUpdating files: 100% (783/783), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"def map_subfolders_to_int(path):\n    reversed_labels = {}\n    counter = 0\n\n    # Walk through the directory\n    for root, dirs, files in os.walk(path):\n        # For each subdirectory\n        for dir in dirs:\n            # Add the subdirectory to the dictionary with the current count as the key\n            reversed_labels[counter] = dir\n            counter += 1\n\n    return reversed_labels\n\n# Specify your directory path here\ndirectory_path = \"/kaggle/working/NBAC/audio\"\nreversed_labels = map_subfolders_to_int(directory_path)\nprint(reversed_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.930783Z","iopub.execute_input":"2023-07-20T10:45:16.931134Z","iopub.status.idle":"2023-07-20T10:45:16.941592Z","shell.execute_reply.started":"2023-07-20T10:45:16.931094Z","shell.execute_reply":"2023-07-20T10:45:16.940112Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{0: 'talking', 1: 'coughing', 2: 'dog bark', 3: 'rain', 4: 'loud breathing', 5: 'siren', 6: 'bed movement', 7: 'traffic', 8: 'wind', 9: 'snoring', 10: 'train', 11: 'sneezing', 12: 'silence'}\n","output_type":"stream"}]},{"cell_type":"code","source":"NUM_CLASSES = len(reversed_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.943842Z","iopub.execute_input":"2023-07-20T10:45:16.944524Z","iopub.status.idle":"2023-07-20T10:45:16.952505Z","shell.execute_reply.started":"2023-07-20T10:45:16.944488Z","shell.execute_reply":"2023-07-20T10:45:16.951488Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def reverse_dict(original_dict):\n    reversed_dict = {value: key for key, value in original_dict.items()}\n    return reversed_dict\n\n# reversed dictionary\nlabels_dict = reverse_dict(reversed_labels)\nprint(labels_dict)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.954131Z","iopub.execute_input":"2023-07-20T10:45:16.954793Z","iopub.status.idle":"2023-07-20T10:45:16.964221Z","shell.execute_reply.started":"2023-07-20T10:45:16.954761Z","shell.execute_reply":"2023-07-20T10:45:16.963199Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'talking': 0, 'coughing': 1, 'dog bark': 2, 'rain': 3, 'loud breathing': 4, 'siren': 5, 'bed movement': 6, 'traffic': 7, 'wind': 8, 'snoring': 9, 'train': 10, 'sneezing': 11, 'silence': 12}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Load the wav files","metadata":{}},{"cell_type":"code","source":"NBAC_FOLDER = '/kaggle/working/NBAC/audio'","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.965798Z","iopub.execute_input":"2023-07-20T10:45:16.966199Z","iopub.status.idle":"2023-07-20T10:45:16.974544Z","shell.execute_reply.started":"2023-07-20T10:45:16.966167Z","shell.execute_reply":"2023-07-20T10:45:16.973652Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def wav_data_loader(directory, sr=SR, normalization=True):\n    all_fragments = []\n    for root, dirs, _ in os.walk(directory):\n        for dir in dirs:\n            dir_path = os.path.join(root, dir)\n            files = [f for f in os.listdir(dir_path) if f.endswith('.wav')]\n            for file in files:\n                file_path = os.path.join(dir_path, file)\n                # Load the audio file\n                sample, sample_rate = librosa.load(file_path, sr=sr)\n                if normalization:\n                    # Normalize the waveform\n                    sample = librosa.util.normalize(sample)\n                # Append the sample and its label (subfolder name) as a tuple\n                all_fragments.append((sample, labels_dict[dir]))\n    return all_fragments","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.976206Z","iopub.execute_input":"2023-07-20T10:45:16.976539Z","iopub.status.idle":"2023-07-20T10:45:16.986739Z","shell.execute_reply.started":"2023-07-20T10:45:16.976508Z","shell.execute_reply":"2023-07-20T10:45:16.985796Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\nnbac_wav_dataset = wav_data_loader(NBAC_FOLDER, normalization=False)\nlen(nbac_wav_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:16.988421Z","iopub.execute_input":"2023-07-20T10:45:16.989149Z","iopub.status.idle":"2023-07-20T10:45:32.755443Z","shell.execute_reply.started":"2023-07-20T10:45:16.989100Z","shell.execute_reply":"2023-07-20T10:45:32.754439Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 13.6 s, sys: 753 ms, total: 14.3 s\nWall time: 15.8 s\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"780"},"metadata":{}}]},{"cell_type":"code","source":"len(nbac_wav_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.757071Z","iopub.execute_input":"2023-07-20T10:45:32.758216Z","iopub.status.idle":"2023-07-20T10:45:32.765724Z","shell.execute_reply.started":"2023-07-20T10:45:32.758180Z","shell.execute_reply":"2023-07-20T10:45:32.764812Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"780"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train-test split\n\nCreating a first train-test split in the original dataset will be useful for correct data augmentation and pre-processing techniques.","metadata":{}},{"cell_type":"code","source":"X = [wav[0] for wav in nbac_wav_dataset]\ny = [wav[1] for wav in nbac_wav_dataset]\n\nX_train_wav, X_test_wav, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\nX_train_wav, X_val_wav, y_train, y_val = train_test_split(X_train_wav, y_train, test_size=0.2, stratify=y_train, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.767962Z","iopub.execute_input":"2023-07-20T10:45:32.768291Z","iopub.status.idle":"2023-07-20T10:45:32.786464Z","shell.execute_reply.started":"2023-07-20T10:45:32.768259Z","shell.execute_reply":"2023-07-20T10:45:32.785362Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Waveform augmentation","metadata":{}},{"cell_type":"code","source":"## Noise addition\n\ndef add_noise(wav_data, noise_factor):\n\n    # Generate noise signal with the same shape as input waveform\n    noise = np.random.normal(0, 1, len(wav_data))\n\n    # Scale noise signal with the permissible noise factor value\n    noise *= noise_factor\n\n    # Add noise signal to input waveform\n    augmented_wav_data = wav_data + noise\n\n    # Normalize the augmented waveform data\n    augmented_wav_data = librosa.util.normalize(augmented_wav_data)\n\n    return augmented_wav_data\n\ndef time_shift(audio, p):\n    \"\"\"\n    Shift audio to the left or right by a random amount.\n    \"\"\"\n    # Calculate the length of the audio array\n    length = audio.shape[0]\n\n    # Calculate the maximum number of samples to shift\n    max_shift = int(length * p)\n\n    # Generate a random shift value\n    shift = random.randint(-max_shift, max_shift)\n\n    # Create an empty array with the same shape as the audio array\n    shifted_audio = np.zeros_like(audio)\n\n    # Shift the audio by the specified number of samples\n    if shift > 0:\n      # Shift to the right\n        shifted_audio[shift:] = audio[:length-shift]\n    else:\n        # Shift to the left\n        shifted_audio[:length+shift] = audio[-shift:]\n    \n    if np.sum(shifted_audio) == 0:\n        #revert the process if all information was erased\n        shifted_audio = audio     \n\n    return shifted_audio\n\ndef time_stretching(audio,factor):\n    \n    wav_time_stch = librosa.effects.time_stretch(audio,rate=factor)\n    \n    return wav_time_stch[:SR*5]","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.787941Z","iopub.execute_input":"2023-07-20T10:45:32.788757Z","iopub.status.idle":"2023-07-20T10:45:32.800592Z","shell.execute_reply.started":"2023-07-20T10:45:32.788720Z","shell.execute_reply":"2023-07-20T10:45:32.799204Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"len(X_train_wav)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.805103Z","iopub.execute_input":"2023-07-20T10:45:32.805439Z","iopub.status.idle":"2023-07-20T10:45:32.812100Z","shell.execute_reply.started":"2023-07-20T10:45:32.805407Z","shell.execute_reply":"2023-07-20T10:45:32.811093Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"436"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n''' EXUCUTE THIS CELL TO APPLY DATA AUGMENTATION\nLots of memory required this step could be optimized'''\n\ndef augment_wavs(wav_dataset, y):\n    \n    y = list(y)\n    \n    wav_dataset_augmented = []\n\n    for wav in wav_dataset:\n        # Create a copy of the original wav to prevent unwanted side effects\n        temp_wav = wav.copy()\n        temp_wav = add_noise(temp_wav, 0.025) # We want to use values between 0.005 and 0.04\n        temp_wav = time_shift(temp_wav, 0.3)  # We want to use a max shift of 30%\n        temp_wav = time_stretching(temp_wav, 0.85)\n\n        wav_dataset_augmented.append(temp_wav)\n\n    # Add original wavs to augmented list\n    wav_dataset_augmented.extend(wav_dataset)\n    \n    y = y + y #each spec is being appended at the bottom of the list\n\n    return wav_dataset_augmented, y","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.813528Z","iopub.execute_input":"2023-07-20T10:45:32.814698Z","iopub.status.idle":"2023-07-20T10:45:32.824527Z","shell.execute_reply.started":"2023-07-20T10:45:32.814581Z","shell.execute_reply":"2023-07-20T10:45:32.823494Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\nWall time: 10.7 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"#X_train_wav, y_train = augment_wavs(X_train_wav, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.826121Z","iopub.execute_input":"2023-07-20T10:45:32.826503Z","iopub.status.idle":"2023-07-20T10:45:32.838120Z","shell.execute_reply.started":"2023-07-20T10:45:32.826463Z","shell.execute_reply":"2023-07-20T10:45:32.837184Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"len(X_train_wav)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.839863Z","iopub.execute_input":"2023-07-20T10:45:32.840222Z","iopub.status.idle":"2023-07-20T10:45:32.849528Z","shell.execute_reply.started":"2023-07-20T10:45:32.840191Z","shell.execute_reply":"2023-07-20T10:45:32.848305Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"436"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Transfer learning with Imagenet","metadata":{}},{"cell_type":"code","source":"# Load the MobileNetV1 model but exclude the top layers\nmobilenet_model = MobileNet(weights='imagenet', include_top=False, input_shape=(498, 64, 3))\n\nclass SpectrogramLayer(Layer):\n    def __init__(self, sample_rate, frame_length, frame_step, num_mel_bins=64, lower_freq=125, upper_freq=7500, log_offset=0.001, **kwargs):\n        super(SpectrogramLayer, self).__init__(**kwargs)\n        self.sample_rate = sample_rate\n        self.frame_length = frame_length\n        self.frame_step = frame_step\n        self.num_mel_bins = num_mel_bins\n        self.lower_freq = lower_freq\n        self.upper_freq = upper_freq\n        self.log_offset = log_offset\n\n    def call(self, inputs):\n        \n        # Convert numpy array to Tensor and normalize based on its actual max and min values\n        wav = tf.cast(inputs, tf.float32)\n        audio_tensor = (wav - tf.math.reduce_min(wav)) / (tf.math.reduce_max(wav) - tf.math.reduce_min(wav)) * 2 - 1\n        \n        # Compute the Short-Time Fourier Transform (STFT)\n        stft = tf.signal.stft(wav, self.frame_length, self.frame_step, window_fn=tf.signal.hann_window)\n\n        # Compute the spectrogram\n        spectrogram = tf.abs(stft)\n\n        # Compute the mel-spectrogram\n        num_spectrogram_bins = stft.shape[-1]\n        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n            self.num_mel_bins, num_spectrogram_bins, self.sample_rate, self.lower_freq, self.upper_freq)\n        mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n        mel_spectrogram.set_shape(spectrogram.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))\n\n        # Compute the log mel-spectrogram\n        log_mel_spectrogram = tf.math.log(mel_spectrogram + self.log_offset)\n\n        # Add a channel dimension\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n        \n        # Convert 1-channel image to 3-channel image\n        x = tf.repeat(log_mel_spectrogram, 3, axis=-1)\n        \n        return x\n\nsample_rate = 16000  # Adjust as necessary\nwindow_size_ms = 25\nwindow_hop_ms = 10\nframe_length = sample_rate * window_size_ms // 1000\nframe_step = sample_rate * window_hop_ms // 1000\naudio_length = sample_rate*5\n\nMobileNet_embeddings = Sequential([\n    mobilenet_model,\n    GlobalAveragePooling2D(),\n])\n\n# Define the model\nmodel = Sequential([\n    SpectrogramLayer(sample_rate=sample_rate, frame_length=frame_length, frame_step=frame_step, input_shape=(audio_length,)),\n    MobileNet_embeddings,\n    tf.keras.layers.Dense(NUM_CLASSES),\n    tf.keras.layers.Activation('softmax')\n])\n\nfor layer in mobilenet_model.layers[:20]: #The first 20 layers of the model are frozen\n    layer.trainable = False\n\n# Define a learning rate\nlearning_rate = 0.001/2 # With low learning rates the model gets stuck at local minumums that cause overfitting.\n                        # With high learning rates the performance is inconsistent\n\n# Initialize the Adam optimizer with the lower learning rate\noptimizer = Adam(learning_rate=learning_rate)\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-20T10:45:32.851485Z","iopub.execute_input":"2023-07-20T10:45:32.851866Z","iopub.status.idle":"2023-07-20T10:45:39.443052Z","shell.execute_reply.started":"2023-07-20T10:45:32.851788Z","shell.execute_reply":"2023-07-20T10:45:39.442041Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n17225924/17225924 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_subsets(X, y, num_samples):\n    X = np.array(X)\n    y = np.array(y)\n    classes = np.unique(y)\n    X_subsets = []\n    y_subsets = []\n    for cls in classes:\n        idx = np.where(y == cls)[0]\n        if len(idx) > num_samples:\n            idx = np.random.choice(idx, num_samples, replace=False)\n        X_subsets.append(X[idx])\n        y_subsets.append(y[idx])\n    X_subset = np.concatenate(X_subsets)\n    y_subset = np.concatenate(y_subsets)\n    X_subset, y_subset = augment_wavs(X_subset, y_subset)\n    return X_subset, y_subset\n\ntest_accuracies = []\nsample_sizes = [6, 10, 14, 18, 22, 26, 30] # INSERT SAMPLE SIZES TO BE TESTED HERE!\n\nfor size in sample_sizes:\n    print(f'Evaluating model trained with {size} samples per class...')\n    # recreate and recompile the model\n    model = Sequential([\n        SpectrogramLayer(sample_rate=sample_rate, frame_length=frame_length, frame_step=frame_step, input_shape=(audio_length,)),\n        MobileNet_embeddings,\n        tf.keras.layers.Dense(NUM_CLASSES),\n        tf.keras.layers.Activation('softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    # Get subset of data and retrain\n    X_train_subset, y_train_subset = create_subsets(X_train_wav, y_train, size)\n    model.fit(\n        np.array(X_train_subset),\n        np.array(y_train_subset),\n        epochs=50,\n        batch_size=32,\n        validation_data=(np.array(X_val_wav), np.array(y_val)),\n        verbose=0  # we don't need epoch-by-epoch output this time\n    )\n\n    # evaluate on the test set\n    test_loss, test_acc = model.evaluate(np.array(X_test_wav), np.array(y_test), verbose=0)\n    test_accuracies.append(test_acc)\n\n# plot test accuracy by training set size\nplt.figure(figsize=(10, 6))\nplt.plot(sample_sizes, test_accuracies, marker='o')\nplt.title('Test accuracy by training set size')\nplt.xlabel('Number of samples per class in training set')\nplt.ylabel('Test accuracy')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}